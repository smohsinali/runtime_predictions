\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{scrextend}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}


\title{ML Training Time Prediction}
\author{ }
\date{April 2017}

\usepackage{natbib}
\usepackage{graphicx}

\begin{document}

\maketitle
\section{Abstract}
In algorithm selection and configuration for a machine learning problem one important thing to consider is if model can be trained in a reasonable time with given hardware resources. In this project we investigate if its possible to predict model training time for Decision Trees, Random Forests and Stochastic Gradient Decent(SGD) which are few of most commonly used machine learning algorithms. We handle this problem of training time prediction as linear regression problem and use Monte Carlo Markov Chain(MCMC) sampling methods to find the closed form solution with uncertainty quantification. We show that for each dataset we can find a good model and predict the total training time with reasonable uncertainty by looking at how long it takes to train the model for few samples of that dataset, but these models are unique for each dataset and one single model cannot be used to predict training time of every dataset.

\section{Introduction}
Training times for machine learning algorithms can vary from few milliseconds to couple of the days depending on different factors. These factors include the algorithm used, its hyperparameters, size of the training data, data structures used in implementation of the algorithm and hardware resources available for the training. If the machine learning scientist knows how much time the training will take he would be able to make more informed decisions about which algorithm to use, which hyperparameters to use, what hardware resources to use and how much data should be used for training model within given time constraints for best performance.\\[10pt]
Some machine learning scientists do complexity analysis of the algorithms and their configurations to see how they will scale to large data sizes but very little work on predicting run times of these algorithms have been done until now. In this project we explore the possibility of predicting run times of three of popular machine learning algorithms: Decision Trees, Random Forests and SGD.\\[10pt]
We understand that predicting exact run times with full certainty is not possible as there are many factors which are unknown or not possible to consider. For example number of background processes, the total resources they consume, the operating system overhead and the processor speeds and architectures are some of such factors. So to consider uncertainties due to such uncontrollable factors in our predictions we solve the regression problem of run time predictions using MCMC methods.

\section{Methodology}
Scikit-learn\citep{pedregosa2011scikit} is a popular open source machine learning library. In this project implementations of Decision Trees, Random Forests and SGD from scikit-learn are used to analyze complexities of these algorithms and get their runtime performances. For building Bayesian model and MCMC sampling we use PyMC3\citep{salvatier2016pymc3} which is python module for Bayesian statistical modeling and model fitting focusing on MCMC algorithms. Specifically following steps are used for predicting runtimes of these algorithms.\\[10pt]
\begin{itemize}
    \item Computational Complexity of algorithm's implementation in scikit-learn is analyzed. This is done by breaking the algorithm in its constituent steps and then for every step analyzing its complexity based on implementation details, E.g if the step being analyzed is sorting operation then we see what sorting algorithm is used for implementing it and what is its computational complexity.
    \item For each algorithm its classifier is trained for 125 different datasets and for each training the datasize and time to train is recorded.
    \item Results of first step are used to create a model in PyMC3 and results of second step are used as observation to find the parameter values of that model.
    \item Finally when the model is fully known it can be used to predict the runtimes. Since MCMC methods are used to find the model parameters we also have access to uncertainties in their values which can be used to find uncertainties in the final predictions.
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%     Decision Trees     %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Analysis of Decision Trees}

\subsection{Algorithm}
Decision trees are among the most popular machine learning algorithms and are also the building blocks of Random Forests. The algorithm of Decision Trees is as follows, For more formal definition look at\citep{louppe2014understanding}:\\
\begin{addmargin}[1em]{0em}
    Create a root node $t_0$ that contains all learning data $L$\\
    Set $currentNode = t_0$\\
	Repeat until stopping criteria is met:
	\begin{itemize}
	    \item Find best split $s^*$ in all variables which maximizes impurity decrease
	    \item Label the $currentNode$ with best split variable and its value
	    \item Divide the available learning data $L$ into $L_l$ and $L_r$
	    \item Create nodes $t_l$ and $t_r$ which contains data $L_l$ and $L_r$ respectively
	    \item Repeat with $currentNode=t_l$ and data $L_l$
	    \item Repeat with $currentNode=t_r$ and data $L_r$
	\end{itemize}
\end{addmargin}

\subsection{Complexity Analysis}
The heart of Decision Trees algorithm computes change in impurity and finds best split based on those measurements. The changes in impurity decrease are measured using impurity function which can be either Gini index or Shannon entropy.\\[10pt]
For each variable finding the best split can be expensive if impurity decrease is evaluated for each change in value of that variable. But if variable is stored in sorted arrays then exhaustive evaluations of all the splits can be carried out efficiently by computing change in impurity from value of change in impurity of previous split in linear number of operations. We know from \citep{} that sorting of arrays is \textit{Lower Bounded}(LB) by:\\[10pt]
                    % \hspace*{5cm}$O(Nlog(N))\qquad \textit{LB of sorting operation}\quad(1)$ \\
                    \hspace*{3cm}$O(Nlog(N)) \quad(1)$ \\[10pt]
So if there are total $k$ number of variable then complexity of induction of Decision Tree would have LB:\\[5pt]
                    \hspace*{3cm}$O(kNlog(N))\quad(2)$ \\
% The Scikit-learn implementation of Decision trees has three main components:
\newpage
% \subsection{Complexity Analysis}
Suppose $T(N)$ is time complexity of building a Decision Tree from $N$ samples. When the  node with $N$ samples is split into two nodes with $N_l$ and $N_r$ samples then the $T(N)$ can be written as sum of cost of finding the split and recursively splitting the children nodes with cost $T(N_l)$ and $T(N_r)$ respectively. This can be written in form of recurrence equation as follows:\\[10pt]
\hspace*{3cm}\begin{math}
  \left\{
    \begin{array}{l}
      T(1) = c_1\\
      T(N) = C(N) + T(N_l) + T(N_r)
    \end{array}
  \right.
\end{math}\quad(3)\\[10pt]
Where $c_1$ is cost of making a leaf node from a sample and $C(N)$ is runtime complexity of finding a split and partitioning $N$ node samples into $t_l$ and $t_r$.
As discussed cost of finding the best split equals to sorting, so from equation(2):\\[10pt]
                $\hspace*{3cm}C(N) = O(kNlog(N)) \quad(4)$\\[10pt]
Equation 3 can be solved for following three cases:
\begin{itemize}
    \item Best case: All nodes of size N are split into two nodes of size $N/2$.
    \item Worst case: All splits are uneven so nodes of size $N$ are split into two nodes with sizes $N-1$ and $1$ respectively.
    \item Average case: Assume that all splits are equiprobable.
\end{itemize}\\[10pt]
From \citep{louppe2014understanding} solving equation 3 for these cases gives following results:\\
\begin{align*}
\textit{Best Case:} & \quad O(kNlog^2N) & \quad (5) \\
\textit{Worst Case:} & \quad O(kN^2logN) & \quad (6) \\
\textit{Avg. Case:} & \quad O(kNlog^2N) & \quad (7) \\
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%     Random Forests   %%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Analysis of Random Forests}

\subsection{Algorithm}
Random Forests are among the most powerful and successfully used machine learning algorithms. Main idea of Random Forests is to build multiple Decision Trees, where each tree is built with subset of variables and bootstrapped data. This ensemble of trees is called Random Forests, it makes prediction by taking outputs of all trees and then averaging the result if its a regression problem or choosing the most popular answer if its a classification problem. The algorithm for creating a Random Forest is as follows, For more formal definition look at\citep{louppe2014understanding}:\\
\begin{addmargin}[1em]{0em}
    Set variable $M=\text{maximum number of trees}$\\
    Set variable $p \leq k$, where $k$ is total number of variables\\
    Set variable $\widetilde{N}=0.632N$ where $N$ is total number of samples\\
    For $1..., M-1, M:$
    \begin{addmargin}[1em]{0em}
    Create a root node $t_0$ that contains bootstraped learning data $L$\\
    Randomly choose $p$ variables
    Set $currentNode = t_0$\\
	Repeat until stopping criteria is met:
	\begin{addmargin}[1em]{0em}    % 	\begin{itemize}
	    Find best split $s^*$ in $p$ variables (randomly chosen in previous step) which maximizes impurity decrease\\
	    Label the $currentNode$ with best split variable and its value\\
	    Divide the available learning data $L$ into $L_l$ and $L_r$\\
	    Create nodes $t_l$ and $t_r$ which contains data $L_l$ and $L_r$ respectively\\
	    Repeat with $currentNode=t_l$ and data $L_l$\\
	    Repeat with $currentNode=t_r$ and data $L_r$\\% 	\end{itemize}
    \end{addmargin}
    \end{addmargin}
\end{addmargin}

\subsection{Complexity Analysis}
The heart of Decision Trees algorithm computes change in impurity and finds best split based on those measurements. The changes in impurity decrease are measured using impurity function which can be either Gini index or Shannon entropy.\\[10pt]
For each variable finding the best split can be expensive if impurity decrease is evaluated for each change in value of that variable. But if variable is stored in sorted arrays then exhaustive evaluations of all the splits can be carried out efficiently by computing change in impurity from value of change in impurity of previous split in linear number of operations. We know from \citep{} that sorting of arrays is \textit{Lower Bounded}(LB) by:\\[10pt]
                    % \hspace*{5cm}$O(Nlog(N))\qquad \textit{LB of sorting operation}\quad(1)$ \\
                    \hspace*{3cm}$O(Nlog(N)) \quad(1)$ \\[10pt]
So if there are total $k$ number of variable then complexity of induction of Decision Tree would have LB:\\[5pt]
                    \hspace*{3cm}$O(kNlog(N))\quad(2)$ \\
% The Scikit-learn implementation of Decision trees has three main components:
\newpage
% \subsection{Complexity Analysis}

From \citep{louppe2014understanding} solving equation 3 for these cases gives following results:\\
\begin{align*}
\textit{Best Case:} & \quad O(kNlog^2N) & \quad (5) \\
\textit{Worst Case:} & \quad O(kN^2logN) & \quad (6) \\
\textit{Avg. Case:} & \quad O(kNlog^2N) & \quad (7) \\
\end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%     SGD  %%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Stochastic Gradient Decent(SGD)}
\subsection{Algorithm}
SGD is an optimization algorithm used to find parameter values of a function such that the value of its cost function is minimized. The idea of the algorithm is very simple, it has the following steps:\\[10pt]
Initialize parameters with random values\\
Repeat until convergence:
\begin{enumerate}
    \item Evaluate the function using current parameter values.
    \item Measure error rate using expected output value and evaluated output value and then update parameter values depending on error rate.
\end{enumerate}\\
In scikit-learn SGD learns parameter values for a linear scoring function $f(x)=w^tx+b$ where $w$ is model parameter and $b$ is y-intercept. The parameter values are optimized by minimizing following error function:\\
	$\hspace*{3cm}E(w,b) = \frac{1}{n} sum(L(y, f(x))) + \alpha R(w) \quad(8)$\\[10pt]
where $L$ is some loss function, $R$ is regularization term that penalizes model complexity and $\alpha$ is a non negative hyperparameter.\\[10pt]

\subsection{Complexity Analysis}
SGD's complexity is linear in number of training examples. Its given by:\\[10pt]
	$\hspace*{3cm} O(kNp) \quad(9)$\\[10pt]
Where $N$ is number of samples, $p$ is number of variables and $k$ is number of iterations.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
After performing the theoretical complexity analysis we test our approach by taking 125 datasets with varying dimensions and performing two different kinds of experiments on them. The data for the experiments is collected by training classifier of given algorithm on each dataset individually and recording how much time it takes to train different sizes of that dataset. For example if dataset has total 100 records then it is recorded that how much time it takes to train the classifier for 10, 20, 30, 40 etc records of that dataset. In this experiment one hypothesis that is also tested is that since 'K'(number of features) remain constant within each dataset it can be factored out from the model without any effect on quality of predictions.\\[10pt]
In the first set of experiments each dataset is taken individually. Then model is trained on it by only looking how much time it took to train first few samples of that data. For example if data has 100 records then we would train model only based on how long it took to train 10 records of that dataset. In the experiments we train model using this approach by taking 9, 13, 19 and 28 percent of data and see how performance of prediction changes. The idea of these experiments is that when user starts training a classifier for machine learning algorithm then he can learn how long will it take to train whole model after only few iterations.\\[10pt]
In the second set of experiments a single model is trained on 100 of 125 datasets and then that model is used to predict running times of rest of 25 datasets individually. The idea of this experiment is that a model is learned from all past runs and then for each new dataset the training time can be instantly predicted just by looking at its dimensions.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}
\subsection{Predictions using initial runtime data}
Results for this set of experiments are obtained by first comparing different models of the given algorithm, After which on one of the better models the difference in quality of predictions due to different amounts of initial data are analyzed.
%%%%%%%%%%%%%%%%%
\subsubsection*{Decision Trees}
As discussed earlier the model for average case of Decision Trees is $a + bKN(LogN)^2$. Since $K$ remains constant for each dataset it can be factored out of model equation, so model becomes $a + bN(LogN)^2$. The model for worst case of Decision Trees training time is $a + bKN^2(LogN)$.
Following boxplot shows the quality of predictions of the given models with 28\% initial data.
\begin{figure}[h!]
\begin{center}
\includegraphics[width=165mm]{dt_models_comparison2.png}
\end{center}
\caption{Quality of Decision Trees Runtime Prediction models}
\label{fig:dt_models}
\end{figure}

The y-axis of the Figure~\ref{fig:dt_models} shows that for each dataset how much its predicted value differed from the true value. The worst case model always over predicted, for about 75\% of datasets it predicted more than double time and for about 25\% data it predicted over three times the true training time. This model never under predicted and for few huge datasets its prediction quality was good.\\
The average case model predicted the training time of about 25\% datasets within 5\% accuracy and 50\% datasets within 15\% accuracy. The median accuracy of this model is about 5\%. It can also be observed that removal of 'K' from the model doesn't affect its predictions because for each individual dataset 'K' is constant.\\[10pt]

% Now that we have idea about prediction quality of the two models we can see how much effect does the quantity of initial runtime data have on quality of the predictions. Figure~\ref{fig:dt_datasize} gives this comparison.

% \begin{figure}[h!]
% \begin{center}
% \includegraphics[width=165mm]{dt_datasize_comparison.png}
% \end{center}
% \caption{Decision Trees Datasize vs Prediction quality}
% \label{fig:dt_datasize}
% \end{figure}
% The median prediction accuracy when 13\% initial data is used is 18\%, it changes to 12\% when 28\% data is used. Although median prediction accuracy is not that bad for small initial datasize of 13\% the upper quartile range and extreme values are very high at 37\% and 78\% respectively. For 28\% initial data these values 23\% and 50\% only. Also using only 9\% of initial data the prediction quality is very poor with median of about 38\% and and extreme value of 120\%.

%%%%%%%%%%%%%%%
\subsubsection*{Random Forests}
The average case model of Random Forests is $a + bMKN(Log(N))^2$, where $M$ is average number of trees and like $K$ it also factors out for individual datasets, so model becomes $a + bN(LogN)^2$. Similarly the model for worst case training time of Random Forests is $a + bKN^2(Log(N))$.
Following boxplot shows the quality of predictions of the given models with 28\% initial data.
\begin{figure}[h!]
\begin{center}
\includegraphics[width=165mm]{rf_models_comparison.png}
\end{center}
\caption{Quality of Random Forests Runtime Prediction models}
\label{fig:rf_models}
\end{figure}

Figure~\ref{fig:rf_models} shows that the worst case model always over predicted, for about 60\% of datasets it predicted more than double time and for about 30\% data it predicted three times the true training time. This model never under predicted but its predictions were not very good, even the best prediction was 25\% different from the true value.\\
The average case model predicted the training time of about 25\% datasets within 3\% accuracy and 80\% datasets within 20\% accuracy. The median accuracy of this model is about 8\%.\\[10pt]
Random Forests and Decision Trees have same model but in Figure~\ref{fig:rf_vs_ds} it can be observed that prediction quality on Random Forests is much better. Even with only 9\% data 75\% of predictions had less than 40\% difference from true values for RF case. Also with only 13\% data RF model had alomost same prediction quality as DT's model have for 28\% data. REASONS?\\[10pt]

%Figure~\ref{fig:rf_datasize} shows how initial datasize effect the prediction quality for Random Forest's model.

\begin{figure}[h!]
\begin{center}
\includegraphics[width=165mm]{rf_vs_dt.png}
\end{center}
\caption{Random Forests vs Decision Trees Prediction quality}
\label{fig:rf_vs_ds}
\end{figure}

%%%%%%%%%%%%%%%%
\subsubsection*{SGD}
The average case model of SGD is $a + bKN$, where $K$ factors out for individual datasets and model becomes $a + bN$.
Figure~\ref{fig:sgd_models} shows the quality of predictions of the given model with 28\% initial data. Overall SGD's model predictions are quite good with median of -2.5\% and over 75\% predictions within 8\% accuracy. The problem with this model is that there are more under predictions than over predictions and 5-6 datasets of high dimensions were under predicted with inaccuracy of 20\%-40\%. Figure~\ref{fig:sgd_datasize} shows effect of initial data size on prediction quality.
\clearpage
\begin{figure}[h!]
\begin{center}
\includegraphics[width=165mm]{sgd_models_comparison.png}
\end{center}
\caption{Quality of SGD Runtime Prediction model}
\label{fig:sgd_models}
\end{figure}


\begin{figure}[h!]
\begin{center}
\includegraphics[width=165mm]{sgd_datasize_comparison.png}
\end{center}
\caption{SGD Datasize vs Prediction quality}
\label{fig:sgd_datasize}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Predictions using single model}







%%%%%%%%%%%%%%%%5%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%555
\begin{figure}[h!]
\centering
\includegraphics[scale=1.7]{universe.jpg}
\caption{The Universe}
\label{fig:universe}
\end{figure}

\section{Conclusion}
``I always thought something was fundamentally wrong with the universe'' \citep{adams1995hitchhiker}

\bibliographystyle{plain}
\bibliography{references}
\end{document}
